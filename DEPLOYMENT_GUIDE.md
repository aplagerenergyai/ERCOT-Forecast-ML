# ERCOT ML Pipeline - Complete Deployment Guide

## ğŸ¯ Overview

This guide covers the complete deployment of the ERCOT DART prediction ML pipeline from development to production.

---

## ğŸ“‹ Deployment Stages

### Stage 1: Local Development âœ…
```bash
# Test feature engineering
python build_features.py

# Test model training
python train_lgbm.py

# Test inference
python score.py
```

### Stage 2: Containerization âœ…
```bash
# Build container
make build

# Run inference server
make run

# Test endpoints
make test
```

### Stage 3: Azure ML Training ğŸš€
```bash
# Build features in cloud
az ml job create --file aml_build_features.yml

# Train models in parallel
az ml job create --file aml_training_pipeline.yml
```

### Stage 4: Production Inference ğŸŒ
```bash
# Push to registry
make deploy

# Deploy to Azure Container Apps
az containerapp create \
  --name ercot-inference \
  --image myregistry.azurecr.io/ercot-ml-pipeline:latest
```

---

## ğŸ—ï¸ Complete Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCES                             â”‚
â”‚                SQL Server (9 ERCOT Tables)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STEP 1: FEATURE ENGINEERING                     â”‚
â”‚            (Azure ML Job + Container)                        â”‚
â”‚                                                              â”‚
â”‚  build_features.py                                          â”‚
â”‚    â†’ Load 9 tables                                          â”‚
â”‚    â†’ Normalize timestamps                                   â”‚
â”‚    â†’ Resample 5-min â†’ hourly                                â”‚
â”‚    â†’ Merge features                                         â”‚
â”‚    â†’ Save parquet                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         workspaceblobstore/features/hourly_features.parquet  â”‚
â”‚         (1-5 GB, ~1M rows, 50+ features)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            STEP 2+3: MODEL TRAINING PIPELINE                 â”‚
â”‚            (Azure ML Pipeline + 3 Parallel Jobs)             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ LightGBM   â”‚  â”‚ XGBoost    â”‚  â”‚ Deep Learningâ”‚         â”‚
â”‚  â”‚ (cpu)      â”‚  â”‚ (cpu)      â”‚  â”‚ (gpu)        â”‚         â”‚
â”‚  â”‚ 10-20 min  â”‚  â”‚ 10-20 min  â”‚  â”‚ 30-60 min    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚        â”‚               â”‚                â”‚                  â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              workspaceblobstore/models/                      â”‚
â”‚                                                              â”‚
â”‚  â€¢ lgbm/lgbm_model.pkl                                      â”‚
â”‚  â€¢ xgb/xgb_model.pkl                                        â”‚
â”‚  â€¢ deep/deep_model.pt                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STEP 4+5: CONTAINERIZED INFERENCE                    â”‚
â”‚         (Docker + FastAPI + Uvicorn)                         â”‚
â”‚                                                              â”‚
â”‚  Docker Container:                                          â”‚
â”‚    â†’ score.py (FastAPI app)                                 â”‚
â”‚    â†’ Model loader                                           â”‚
â”‚    â†’ Preprocessing pipeline                                 â”‚
â”‚    â†’ Endpoints: /health, /score, /model/info                â”‚
â”‚                                                              â”‚
â”‚  Deployment Options:                                        â”‚
â”‚    â€¢ Azure Container Apps                                   â”‚
â”‚    â€¢ Azure Kubernetes Service                               â”‚
â”‚    â€¢ Azure Container Instances                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PRODUCTION ENDPOINT                         â”‚
â”‚          https://ercot-inference.azurecontainerapps.io       â”‚
â”‚                                                              â”‚
â”‚  Client Application â†’ POST /score â†’ DART Predictions        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Deployment Commands

### Complete End-to-End (First Time)

```bash
# 1. Setup environment
cp .env.example .env
# Edit .env with SQL credentials

# 2. Build features (Azure ML)
az ml job create --file aml_build_features.yml --web

# 3. Train models (Azure ML)
az ml job create --file aml_training_pipeline.yml --web

# 4. Build and push container
make build
make tag REGISTRY=myregistry.azurecr.io
make acr-login
make push

# 5. Deploy to Azure Container Apps
az containerapp create \
  --name ercot-inference \
  --resource-group myresourcegroup \
  --image myregistry.azurecr.io/ercot-ml-pipeline:latest \
  --target-port 5001 \
  --ingress external \
  --env-vars MODEL_TYPE=lgbm \
  --cpu 2 --memory 4Gi

# 6. Test production endpoint
ENDPOINT=$(az containerapp show --name ercot-inference \
  --resource-group myresourcegroup \
  --query properties.configuration.ingress.fqdn -o tsv)

curl https://$ENDPOINT/health
```

### Retraining & Redeployment

```bash
# 1. Retrain models (monthly/quarterly)
az ml job create --file aml_training_pipeline.yml --web

# 2. Rebuild container with new models
make build-no-cache
make deploy

# 3. Update Azure Container App
az containerapp update \
  --name ercot-inference \
  --resource-group myresourcegroup \
  --image myregistry.azurecr.io/ercot-ml-pipeline:latest
```

---

## ğŸ“Š Deployment Checklist

### Pre-Deployment

- [ ] `.env` file created with SQL credentials
- [ ] Azure ML workspace configured
- [ ] Compute clusters created (cpu-cluster, gpu-cluster)
- [ ] Azure Container Registry created
- [ ] Service principal created (for CI/CD)

### Feature Engineering

- [ ] `aml_build_features.yml` job succeeds
- [ ] `hourly_features.parquet` exists in workspaceblobstore
- [ ] Parquet file has expected row count (~1M+)
- [ ] All 50+ features present

### Model Training

- [ ] `aml_training_pipeline.yml` job succeeds
- [ ] All 3 models trained successfully
- [ ] Test RMSE < $10/MWh for all models
- [ ] Test RÂ² > 0.65 for all models
- [ ] Models saved in workspaceblobstore

### Containerization

- [ ] Dockerfile builds successfully
- [ ] Container runs locally without errors
- [ ] Health endpoint returns 200
- [ ] Scoring endpoint returns predictions
- [ ] Test script passes all checks

### Production Deployment

- [ ] Container pushed to ACR
- [ ] Azure Container App created
- [ ] Endpoint accessible publicly (or within VNet)
- [ ] Load balancing configured
- [ ] Autoscaling enabled
- [ ] Monitoring configured
- [ ] Alerts set up

---

## ğŸ”„ CI/CD Pipeline

### GitHub Actions Workflow

Automatically triggers on:
- Push to `main` branch
- Pull request to `main`
- Manual trigger (`workflow_dispatch`)

Pipeline steps:
1. Build Docker image
2. Run security scan (Trivy)
3. Push to Azure Container Registry
4. Test inference endpoint
5. Trigger Azure ML pipeline (optional)
6. Deploy to staging (optional)

### Setup

1. **Create GitHub Secrets**:
   - `ACR_USERNAME`
   - `ACR_PASSWORD`
   - `AZURE_CREDENTIALS`
   - `AZURE_RESOURCE_GROUP`
   - `AZURE_ML_WORKSPACE`

2. **Enable Workflow**:
   ```bash
   # Commit workflow file
   git add .github/workflows/build_and_push.yml
   git commit -m "Add CI/CD pipeline"
   git push origin main
   ```

3. **Monitor**:
   - GitHub Actions tab
   - Azure ML Studio (for training jobs)
   - Azure Portal (for container apps)

---

## ğŸ“ˆ Monitoring & Observability

### Application Insights Integration

Add to Dockerfile:
```dockerfile
ENV APPLICATIONINSIGHTS_CONNECTION_STRING=$APPINSIGHTS_CONN_STRING
RUN pip install opencensus-ext-azure
```

Update score.py:
```python
from opencensus.ext.azure.log_exporter import AzureLogHandler

logger.addHandler(AzureLogHandler(
    connection_string=os.environ['APPLICATIONINSIGHTS_CONNECTION_STRING']
))
```

### Key Metrics to Monitor

- **Inference Metrics**:
  - Request rate (requests/second)
  - Latency (p50, p95, p99)
  - Error rate (%)
  - Prediction distribution

- **Model Metrics**:
  - Model version
  - Feature drift
  - Prediction drift
  - Data quality

- **System Metrics**:
  - CPU utilization
  - Memory usage
  - Disk I/O
  - Network traffic

### Alerts

Set up alerts for:
- Error rate > 5%
- Latency p95 > 1000ms
- CPU > 80% for 5 minutes
- Memory > 90%
- Model drift detected

---

## ğŸ” Security Considerations

### 1. API Authentication

Add API key authentication:
```python
from fastapi import Security, HTTPException
from fastapi.security import APIKeyHeader

api_key_header = APIKeyHeader(name="X-API-Key")

@app.post("/score")
async def score(request: PredictionRequest, api_key: str = Security(api_key_header)):
    if api_key != os.environ["API_KEY"]:
        raise HTTPException(status_code=403, detail="Invalid API key")
    # ... scoring logic
```

### 2. HTTPS Only

Enable HTTPS in Azure Container Apps:
```bash
az containerapp ingress enable \
  --name ercot-inference \
  --type external \
  --allow-insecure false
```

### 3. Network Isolation

Deploy in VNet:
```bash
az containerapp create \
  --name ercot-inference \
  --environment myenvironment \
  --vnet-name myvnet \
  --subnet mysubnet
```

### 4. Managed Identity

Use managed identity for Azure resources:
```bash
az containerapp identity assign \
  --name ercot-inference \
  --resource-group myresourcegroup \
  --system-assigned
```

---

## ğŸ’° Cost Optimization

### Compute Optimization

| Resource | Standard | Optimized | Savings |
|----------|----------|-----------|---------|
| Training Compute | 4x D4s_v3 (always on) | Spot instances, auto-scale | 70% |
| Inference Compute | 3 replicas (always on) | Autoscale 1-5 replicas | 40% |
| GPU Training | 1x NC6 (always on) | Use on-demand | 60% |

### Storage Optimization

- Use lifecycle policies to archive old parquet files
- Compress models before storage
- Use cool storage for historical data

### Total Estimated Monthly Cost

| Component | Monthly Cost |
|-----------|--------------|
| Azure ML Compute (training) | $100-200 |
| Container Apps (inference) | $50-150 |
| Storage (blobs) | $5-20 |
| Container Registry | $5 |
| Networking | $10-30 |
| **Total** | **$170-405/month** |

*(Costs vary based on usage, region, and spot instance availability)*

---

## ğŸ› Common Issues & Solutions

### Issue: Inference is slow

**Symptoms**: Latency > 1000ms

**Solutions**:
- Use LightGBM instead of Deep Learning (5ms vs 15ms)
- Enable model caching
- Add more replicas
- Use faster CPU/GPU

### Issue: Models become stale

**Symptoms**: Prediction accuracy drops over time

**Solutions**:
- Schedule monthly retraining
- Monitor prediction drift
- Set up automated retraining pipeline
- Use online learning (if applicable)

### Issue: High costs

**Symptoms**: Monthly Azure bill > $500

**Solutions**:
- Use spot instances for training
- Enable autoscaling (scale to zero when idle)
- Archive old data to cool storage
- Optimize container image size

### Issue: Container crashes

**Symptoms**: Container repeatedly restarts

**Solutions**:
- Check logs: `docker logs ercot-ml-pipeline`
- Increase memory limit
- Verify model files are accessible
- Check environment variables

---

## ğŸ“š Documentation Index

| Document | Purpose |
|----------|---------|
| **QUICK_START.md** | Fast execution guide |
| **PIPELINE_GUIDE.md** | Complete pipeline documentation |
| **PROJECT_STRUCTURE.md** | Architecture overview |
| **STEP2_SUMMARY.md** | Training implementation |
| **STEP3_PIPELINE.md** | Pipeline orchestration |
| **STEP5_CONTAINERIZATION.md** | Container deployment |
| **DEPLOYMENT_GUIDE.md** | This document |
| **COMPLETE_PIPELINE_SUMMARY.md** | Master summary |

---

## âœ… Production Readiness Checklist

### Code Quality
- [ ] All linter errors resolved
- [ ] Unit tests passing
- [ ] Integration tests passing
- [ ] Documentation complete
- [ ] Code reviewed

### Performance
- [ ] Inference latency < 100ms
- [ ] Throughput > 100 req/s
- [ ] Model accuracy meets requirements
- [ ] Load testing completed

### Reliability
- [ ] Health checks implemented
- [ ] Error handling complete
- [ ] Logging configured
- [ ] Monitoring enabled
- [ ] Alerts configured

### Security
- [ ] Secrets in environment variables
- [ ] HTTPS enabled
- [ ] Authentication implemented
- [ ] Vulnerability scan clean
- [ ] RBAC configured

### Operations
- [ ] CI/CD pipeline working
- [ ] Backup strategy defined
- [ ] Disaster recovery plan
- [ ] Runbook documented
- [ ] On-call rotation setup

---

## ğŸ“ Training & Knowledge Transfer

### For Data Scientists

1. **Model Development**: Use Jupyter notebooks for experimentation
2. **Feature Engineering**: Update `build_features.py` for new features
3. **Model Training**: Modify hyperparameters in training scripts
4. **Evaluation**: Use metrics.py for consistent evaluation

### For ML Engineers

1. **Pipeline**: Understand `aml_training_pipeline.yml` structure
2. **Containerization**: Know Docker and Dockerfile
3. **Deployment**: Understand Azure Container Apps
4. **Monitoring**: Set up Application Insights

### For DevOps

1. **CI/CD**: GitHub Actions workflow
2. **Infrastructure**: Azure resources (AML, ACR, ACA)
3. **Monitoring**: Application Insights, Azure Monitor
4. **Troubleshooting**: Logs, metrics, alerts

---

## ğŸ“ Support & Escalation

### Tier 1: Self-Service
- Check documentation
- Review logs
- Test health endpoint
- Run test script

### Tier 2: Team Support
- Slack channel: #ercot-ml-support
- Email: ml-team@company.com
- Office hours: Mon-Fri 9am-5pm

### Tier 3: On-Call
- PagerDuty: ERCOT ML Oncall
- Phone: +1-xxx-xxx-xxxx
- Escalation: CTO

---

## ğŸ¯ Roadmap

### Q1 2025
- [ ] Deploy to production
- [ ] Set up monitoring
- [ ] Establish retraining schedule
- [ ] Document operational procedures

### Q2 2025
- [ ] Add feature drift detection
- [ ] Implement A/B testing
- [ ] Optimize inference performance
- [ ] Add more settlement points

### Q3 2025
- [ ] Multi-model ensemble
- [ ] Real-time feature updates
- [ ] Advanced monitoring dashboard
- [ ] Automated model selection

### Q4 2025
- [ ] Geographic expansion
- [ ] New forecast horizons
- [ ] Integration with trading systems
- [ ] Advanced anomaly detection

---

**ğŸŠ Deployment guide complete! You're ready for production!**

Start with: `make build && make test && make deploy`

