# Step 2: ML Training Pipeline - Implementation Summary

## âœ… Files Created

### Python Training Scripts (5 files)
1. **`dataloader.py`** (250 lines)
   - `ERCOTDataLoader` class for data loading and preprocessing
   - Time-based train/val/test split (80/10/10)
   - Target creation: DART = DAM_Price_Hourly - RTM_LMP_HourlyAvg
   - Categorical encoding using TargetEncoder
   - Feature standardization using StandardScaler
   - Automatic feature identification (categorical vs continuous)

2. **`metrics.py`** (90 lines)
   - `calculate_rmse()`: Root Mean Squared Error
   - `calculate_mae()`: Mean Absolute Error
   - `calculate_mape()`: Mean Absolute Percentage Error
   - `calculate_r2()`: R-squared score
   - `evaluate_model()`: Comprehensive evaluation
   - MLflow integration for metric logging

3. **`train_lgbm.py`** (130 lines)
   - LightGBM gradient boosting regressor
   - Early stopping with 50-round patience
   - Feature importance analysis (top 20)
   - Model serialization with metadata
   - Saves to: `outputs/lgbm_model.pkl`

4. **`train_xgb.py`** (130 lines)
   - XGBoost extreme gradient boosting
   - Early stopping with 50-round patience
   - Feature importance by gain
   - Model serialization with metadata
   - Saves to: `outputs/xgb_model.pkl`

5. **`train_deep.py`** (180 lines)
   - PyTorch LSTM regression model
   - Architecture: LSTM (2 layers, 128 hidden dim) â†’ FC layers
   - GPU support with automatic device detection
   - Learning rate scheduling (ReduceLROnPlateau)
   - Early stopping with 10-epoch patience
   - Saves to: `outputs/deep_model.pt`

### Azure ML Job Definitions (4 YAML files)

6. **`aml_train_lgbm.yml`**
   - Compute: cpu-cluster
   - Input: features from workspaceblobstore
   - Output: model to workspaceblobstore/models/lgbm/

7. **`aml_train_xgb.yml`**
   - Compute: cpu-cluster
   - Input: features from workspaceblobstore
   - Output: model to workspaceblobstore/models/xgb/

8. **`aml_train_deep.yml`**
   - Compute: gpu-cluster
   - Input: features from workspaceblobstore
   - Output: model to workspaceblobstore/models/deep/

9. **`aml_training_pipeline.yml`**
   - Parallel execution of all 3 training jobs
   - Shared input: features folder
   - Separate outputs for each model

### Supporting Files

10. **`requirements.txt`**
    - Complete Python dependencies
    - pandas, numpy, pyarrow
    - pyodbc, sqlalchemy
    - lightgbm, xgboost
    - torch, category-encoders
    - azureml-core

11. **`environment.yml`** (updated)
    - Added category-encoders dependency

12. **`PIPELINE_GUIDE.md`**
    - Complete user documentation
    - Step-by-step instructions
    - Troubleshooting guide
    - Architecture overview

---

## ğŸ”„ Updated Files

### `build_features.py`
**Fixed critical merge issue**:
- Original version only merged global features (load, solar, wind)
- **Updated** to properly merge settlement point prices (DAM + RTM)
- Now creates correct grain: one row per hour per settlement point
- Enables DART target calculation per settlement point

---

## ğŸ¯ Key Features Implemented

### Data Loading
- âœ… Loads parquet from Azure ML workspaceblobstore
- âœ… Automatic detection of feature types (categorical vs continuous)
- âœ… Time-based split (no data leakage)
- âœ… Target creation with missing value handling

### Feature Engineering
- âœ… **Categorical Encoding**: TargetEncoder for SettlementPoint
- âœ… **Standardization**: Z-score normalization (fit on train only)
- âœ… **Missing Value Handling**: Fill with 0 after standardization
- âœ… Proper train/val/test transformation pipeline

### Model Training
- âœ… **LightGBM**: Optimized hyperparameters, early stopping
- âœ… **XGBoost**: Tree-based regression, early stopping
- âœ… **Deep Learning**: LSTM with dropout, GPU support
- âœ… All models save with complete metadata

### Evaluation
- âœ… RMSE, MAE, MAPE, RÂ² on all three sets
- âœ… Feature importance analysis (LightGBM, XGBoost)
- âœ… MLflow integration for experiment tracking

### Azure ML Integration
- âœ… Environment variable handling (`AZUREML_INPUT_*`, `AZUREML_OUTPUT_*`)
- âœ… Proper input/output path configuration
- âœ… Parallel pipeline execution
- âœ… Separate compute for CPU vs GPU workloads

---

## ğŸ“Š Data Pipeline Flow

```
Step 1: Feature Engineering
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Server (9 ERCOT Tables)             â”‚
â”‚ - Load tables (forecast + weather zones)â”‚
â”‚ - Price tables (DAM + RTM)              â”‚
â”‚ - Solar/Wind generation & forecasts     â”‚
â”‚ - SCED constraints                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
        build_features.py
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ hourly_features.parquet                 â”‚
â”‚ - TimestampHour (datetime)              â”‚
â”‚ - SettlementPoint (categorical)         â”‚
â”‚ - DAM_Price_Hourly (continuous)         â”‚
â”‚ - RTM_LMP_HourlyAvg (continuous)        â”‚
â”‚ - 50+ engineered features               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
Step 2: Model Training (Parallel)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         dataloader.py                   â”‚
â”‚ 1. Load parquet                         â”‚
â”‚ 2. Create DART target                   â”‚
â”‚ 3. Time-based split (80/10/10)          â”‚
â”‚ 4. Encode SettlementPoint               â”‚
â”‚ 5. Standardize continuous features      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â†“                 â†“              â†“
train_lgbm.py   train_xgb.py   train_deep.py
    â†“                 â†“              â†“
lgbm_model.pkl  xgb_model.pkl  deep_model.pt
```

---

## ğŸš€ How to Run

### Option 1: Individual Jobs

```bash
# Step 1: Build features
az ml job create --file aml_build_features.yml --web

# Step 2a: Train LightGBM
az ml job create --file aml_train_lgbm.yml --web

# Step 2b: Train XGBoost
az ml job create --file aml_train_xgb.yml --web

# Step 2c: Train Deep Learning
az ml job create --file aml_train_deep.yml --web
```

### Option 2: Full Pipeline (Recommended)

```bash
# Step 1: Build features
az ml job create --file aml_build_features.yml --web

# Wait for completion, then...

# Step 2: Train all models in parallel
az ml job create --file aml_training_pipeline.yml --web
```

---

## ğŸ“ˆ Expected Results

### Dataset Statistics
- **Rows**: ~1,000,000+ (hours Ã— ~1053 settlement points)
- **Features**: ~50+ engineered features
- **Target**: DART spread ($/MWh)
- **Train**: 80% earliest data
- **Val**: 10% middle period
- **Test**: 10% most recent data

### Model Performance (estimated)
Based on typical DART spread prediction:
- **LightGBM**: RMSE ~$3-5/MWh, RÂ² ~0.70-0.85
- **XGBoost**: RMSE ~$3-5/MWh, RÂ² ~0.70-0.85
- **Deep Learning**: RMSE ~$4-6/MWh, RÂ² ~0.65-0.80

(Actual performance depends on data quality and time period)

---

## ğŸ”§ Customization Points

### Hyperparameter Tuning
All hyperparameters are defined in the training scripts:
- `train_lgbm.py`: lines 30-42 (num_leaves, learning_rate, etc.)
- `train_xgb.py`: lines 30-43 (max_depth, learning_rate, etc.)
- `train_deep.py`: lines 130-140 (hidden_dim, num_layers, dropout)

### Train/Val/Test Split
Modify `dataloader.py` line 83:
```python
train_pct=0.8, val_pct=0.1, test_pct=0.1
```

### Categorical Encoding Method
Change in `dataloader.py` line 138:
- Current: `TargetEncoder` (smooth mean encoding)
- Alternative: `OrdinalEncoder`, `CatBoostEncoder`

### Feature Selection
Modify `identify_feature_columns()` in `dataloader.py` to:
- Exclude specific features
- Add custom feature transformations
- Filter by importance

---

## âœ… Validation Checklist

- [x] All Python scripts have proper error handling
- [x] Azure ML environment variables are handled correctly
- [x] Time-based split prevents data leakage
- [x] Categorical encoding fits on train set only
- [x] Standardization fits on train set only
- [x] All models save with complete metadata
- [x] Metrics are computed for train/val/test
- [x] Feature importance is logged
- [x] GPU support for deep learning
- [x] Early stopping prevents overfitting
- [x] Missing values are handled properly
- [x] Target variable (DART) is correctly calculated

---

## ğŸ“ Notes

### Critical Fix Applied
The original `build_features.py` had a merge issue where settlement point prices (DAM and RTM) were not included in the final output. This has been **fixed** - the merge now:
1. Starts with DAM prices (defines grain)
2. Merges RTM prices on (TimestampHour, SettlementPoint)
3. Broadcasts global features (load, solar, wind) on TimestampHour

This ensures the DART target can be calculated correctly.

### Compute Requirements
- **Feature Engineering**: 4-8 cores, 16-32 GB RAM, ~30-60 min
- **LightGBM/XGBoost**: 4-8 cores, 8-16 GB RAM, ~10-20 min each
- **Deep Learning**: 1-2 GPUs (V100/A100), 16-32 GB VRAM, ~30-60 min

### Storage
- **Input**: `hourly_features.parquet` (~1-5 GB)
- **Output Models**: 
  - LightGBM: ~50-200 MB
  - XGBoost: ~50-200 MB
  - Deep Learning: ~10-50 MB

---

## ğŸ“ Next Steps

1. **Run Feature Engineering**: Submit `aml_build_features.yml`
2. **Verify Output**: Check that `hourly_features.parquet` exists in workspaceblobstore
3. **Train Models**: Submit `aml_training_pipeline.yml`
4. **Compare Results**: Review metrics for all three models
5. **Select Best Model**: Based on test set performance
6. **Deploy**: Register and deploy the best model to an endpoint

---

## ğŸ“ Support

See `PIPELINE_GUIDE.md` for detailed troubleshooting and customization options.

